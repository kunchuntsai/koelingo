# KoeLingo - Japanese Speech Recognition and Translation App Design

## Table of Contents
1. [User Journey](#1-user-journey)
   1. [High-level](#11-high-level)
   2. [Step-by-step User Flow](#12-step-by-step-user-flow)
   3. [Technical Requirements](#13-technical-requirements)
2. [System Architecture](#2-system-architecture)
   1. [Dataflow Diagram](#21-dataflow-diagram)
   2. [Building Blocks Diagram](#22-building-blocks-diagram)
   3. [Modules & Libraries](#23-modules--libraries)
3. [Functional Requirements](#3-functional-requirements)
   1. [Audio Capture](#31-audio-capture)
   2. [Speech Recognition](#32-speech-recognition)
   3. [Translation](#33-translation)
   4. [User Interface](#34-user-interface)
   5. [Data Storage](#35-data-storage)
4. [Non-Functional Requirements](#4-non-functional-requirements)
   1. [Performance](#41-performance)
   2. [Reliability](#42-reliability)
   3. [Usability](#43-usability)
5. [UI Design Specifications](#5-ui-design-specifications)
   1. [Main Window](#51-main-window)
   2. [Visual Indicators](#52-visual-indicators)
   3. [Settings Panel](#53-settings-panel)
6. [Component Architecture](#6-component-architecture)
   1. [Audio Module](#61-audio-module)
   2. [Speech-to-Text Module](#62-speech-to-text-module)
   3. [Translation Module](#63-translation-module)
   4. [UI Module](#64-ui-module)
   5. [Storage Module](#65-storage-module)
7. [Benchmarking Strategy](#7-benchmarking-strategy)
   1. [Audio Processing Metrics](#71-audio-processing-metrics)
   2. [Speech-to-Text Performance](#72-speech-to-text-performance)
   3. [Translation Benchmarks](#73-translation-benchmarks)
   4. [End-to-End System Tests](#74-end-to-end-system-tests)
8. [Implementation](#8-implementation)
9. [Future Enhancements](#9-future-enhancements)
   1. [Short-term](#91-short-term)
   2. [Long-term](#92-long-term)
10. [Development Environment Setup](#10-development-environment-setup)
    1. [Required Tools](#101-required-tools)
    2. [Development Workflow](#102-development-workflow)

## 1. User Journey

### 1.1 High-level
KoeLingo is a MacBook Pro desktop application that listens to Japanese speech and provides:
- Real-time Japanese speech-to-text transcription with UI display
- Real-time translation from Japanese to English with UI display
- Ability to save both Japanese and English transcripts to a document when closed

### 1.2 Step-by-step User Flow
- **Start Listening Button**: Activates microphone and begins speech-to-text processing
- **Stop Listening Button**: Deactivates microphone and pauses speech-to-text
- **Japanese Text Window**: Displays real-time transcription of Japanese speech
- **English Text Window**: Displays real-time translation of the Japanese text
- **Save & Close Button**: Saves both Japanese and English transcripts to a document before terminating the application

The application continues listening and processing speech as long as the "Start Listening" function is active.

### 1.3 Technical Requirements
- Local LLM deployment for speech recognition and translation
- Optimized for MacBook Pro with M4 chip
- Real-time processing with minimal latency

## 2. System Architecture

### 2.1 Dataflow Diagram

```
┌─────────────┐     ┌───────────────┐     ┌───────────────┐     ┌──────────────────┐
│             │     │               │     │               │     │                  │
│  Microphone │────▶│ Audio Capture │────▶│  Audio Buffer │────▶│  Audio Processor │
│             │     │   Module      │     │   (16kHz)     │     │  (Noise Filters) │
└─────────────┘     └───────────────┘     └───────────────┘     └──────────────────┘
                                                                          │
                                                                          ▼
┌─────────────────────┐     ┌───────────────┐              ┌───────────────────────┐
│                     │     │               │              │                       │
│ Japanese Text Panel │◀────│ Text Formatter│◀─────────────│ Whisper STT Model     │
│                     │     │               │              │ (Japanese Recognition) │
└─────────────────────┘     └───────────────┘              └───────────────────────┘
        │                                                              │
        │                                                              │
        │                                                              ▼
        │                   ┌───────────────┐              ┌───────────────────────┐
        │                   │               │              │                       │
        └───────────────────▶ Storage Module◀──────────────│ NLLB Translation      │
                            │               │              │ Model                 │
                            └───────────────┘              └───────────────────────┘
                                    ▲                                  ▲
                                    │                                  │
                                    │                                  │
                            ┌───────────────────────┐                  │
                            │                       │                  │
                            │  English Text Panel   │──────────────────┘
                            │                       │
                            └───────────────────────┘
                                    │
                                    ▼
                            ┌───────────────┐
                            │               │
                            │  Saved Files  │
                            │  (Transcripts)│
                            └───────────────┘
```

#### 2.1.1 Detailed Data Flow
1. **Audio Capture Flow**:
   - Microphone captures raw audio (analog signal)
   - Audio Capture Module digitizes the signal at 16kHz sample rate
   - Audio Buffer stores chunks of audio data (configurable buffer size, default: 30-second chunks with 5-second overlaps)
   - Audio Processor applies noise filtering and signal normalization

2. **Speech Recognition Flow**:
   - Processed audio chunks are fed to Whisper model
   - Whisper performs Japanese speech recognition
   - Recognized text is formatted with timestamps and confidence metrics
   - Formatted Japanese text is displayed in the Japanese Text Panel
   - Recognized text segments are also sent to Translation module

3. **Translation Flow**:
   - Japanese text segments are processed by NLLB translation model
   - NLLB translates Japanese to English
   - Translated text is formatted with timestamps and confidence metrics
   - Formatted English text is displayed in the English Text Panel

4. **Storage Flow**:
   - Both Japanese and English texts are periodically saved to temporary storage (auto-save)
   - On "Save & Close", both texts are saved to permanent storage
   - Saved files include metadata such as timestamps, speakers (if identified), and confidence scores

### 2.2 Building Blocks Diagram

```
┌────────────────────────────────────────────────────────────┐
│                        KoeLingo                            │
│                                                            │
│  ┌───────────────┐   ┌───────────────┐   ┌───────────────┐ │
│  │               │   │               │   │               │ │
│  │  UI Module    │   │ Audio Module  │   │  STT Module   │ │
│  │               │   │               │   │               │ │
│  └───────────────┘   └───────────────┘   └───────────────┘ │
│                                                            │
│  ┌───────────────┐   ┌───────────────┐   ┌───────────────┐ │
│  │               │   │               │   │               │ │
│  │ Translation   │   │ Storage       │   │ Utils/Config  │ │
│  │ Module        │   │ Module        │   │ Module        │ │
│  │               │   │               │   │               │ │
│  └───────────────┘   └───────────────┘   └───────────────┘ │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

#### 2.2.1 Top-Level Module Responsibilities

1. **UI Module** (Python/PySide6)
   - Displays Japanese and English text panels
   - Provides audio control interface
   - Visualizes audio levels and confidence metrics

2. **Audio Module** (C++ with Python bindings)
   - Captures and processes microphone input
   - Implements efficient audio buffering
   - Performs noise filtering and preprocessing

3. **STT Module** (Python frontend, C++ optimized inference)
   - Manages Whisper model for Japanese speech recognition
   - Processes audio to generate text with timestamps
   - Uses CTranslate2 for optimized inference

4. **Translation Module** (Python frontend, C++ optimized inference)
   - Manages NLLB model for Japanese-English translation
   - Maintains context across sentence boundaries
   - Uses CTranslate2 for optimized inference

5. **Storage Module** (Python)
   - Handles transcript saving and auto-saving
   - Manages file exports in multiple formats
   - Implements session recovery mechanisms

6. **Utils/Config Module** (Python)
   - Manages application settings and configuration
   - Provides cross-language communication utilities
   - Handles logging and error management

#### 2.2.2 Module Communication

- **Python-to-C++ Data Flow**: Optimized through chunked data transfers and efficient memory management
- **Performance-Critical Paths**: Audio → STT → Translation implemented with minimal language boundary crossings
- **Threading Model**: Audio processing and model inference run in separate C++ threads, UI in main Python thread
- **UI Updates**: Asynchronous signaling from C++ components to Python UI through event callbacks

### 2.3 Modules & Libraries

#### 2.3.1 Language Choice: Hybrid Python/C++ Approach

After careful consideration, a hybrid Python/C++ approach has been selected as the optimal solution:

**Core Architecture**
- **Python**: Primary application language, UI development, high-level logic
- **C++**: Performance-critical components, audio processing, model inference optimization

**Rationale for Hybrid Approach**
- Balances development speed with performance requirements
- Leverages Python's ML/AI ecosystem while maintaining C++ performance where needed
- Allows selective optimization of bottlenecks without rewriting the entire application
- Provides flexibility to evolve the application over time

**Implementation Strategy**
- **Python Components**:
  - Main application logic and UI (using PySide6/PyQt6)
  - Model loading and configuration
  - File I/O and document management
  - Application settings and state management

- **C++ Components**:
  - Real-time audio capture and preprocessing
  - Fast buffer management for audio data
  - Performance-critical parts of model inference
  - Any identified processing bottlenecks

- **Integration Mechanism**:
  - Python C++ extensions using pybind11 for custom components
  - Pre-built optimized libraries with Python bindings (CTranslate2)
  - Efficient data passing strategies to minimize language boundary overhead

**Development Workflow**
1. Implement initial prototype entirely in Python for rapid iteration
2. Profile application to identify performance bottlenecks
3. Selectively implement critical components in C++
4. Integrate C++ components via Python bindings
5. Iterate based on performance benchmarking

#### 2.3.2 UI Framework: Qt via Python

Qt (through PySide6) has been selected as the UI framework due to:

- **Rich Audio Processing Support**: Built-in components for audio visualization and processing
- **Excellent Text Handling**: Superior multilingual text support for Japanese/English display
- **Cross-Language Consistency**: Same API in both Python (PySide6) and C++ (Qt)
- **Extensive Ecosystem**: Large user base, especially in audio applications and Asian markets
- **Performance**: Optimized for applications with real-time updates and long running sessions

**Qt Components to Leverage**
- QAudioInput for microphone capture
- QMediaPlayer for audio processing
- QTextEdit with rich text formatting for transcript display
- QCustomPlot for audio waveform visualization
- Qt Linguist for internationalization support

#### 2.3.3 Core Libraries

- **UI**:
  - PySide6/PyQt6 (Python bindings for Qt)
  - Qt Quick for modern UI components

- **Audio Processing**:
  - PyAudio with PortAudio backend (Python interface, C++ core)
  - Optional: Custom C++ audio processing modules for enhanced performance

- **Speech-to-Text**:
  - Whisper (OpenAI's speech recognition model)
  - CTranslate2 for optimized Whisper inference (C++ backend with Python bindings)

- **Translation**:
  - NLLB (No Language Left Behind) for Japanese-English translation
  - CTranslate2 for optimized NLLB inference

- **Data Processing**:
  - NumPy for efficient numerical operations
  - Python standard library for file I/O
  - Optional: Custom C++ modules for performance-critical data processing

- **Development Tools**:
  - pybind11 for creating Python bindings to C++ code
  - CMake for C++ build configuration
  - Poetry/Pipenv for Python dependency management

#### 2.3.4 Performance Optimization Strategy

To ensure optimal performance in the hybrid approach:

1. **Minimize Language Boundary Crossings**:
   - Process data in larger chunks when passing between Python and C++
   - Keep tight processing loops entirely within one language

2. **Prioritize C++ Implementation For**:
   - Audio capture and buffering (continuous real-time operation)
   - Audio preprocessing (noise filtering, normalization)
   - Performance-critical inference paths

3. **Leverage Existing Optimized Libraries**:
   - CTranslate2 for model inference (C++ optimized with Python bindings)
   - PortAudio for audio capture (C core with Python bindings)

4. **Threading Strategy**:
   - Audio processing in dedicated C++ thread
   - Model inference in separate threads
   - UI updates in main Python thread
   - Thread-safe queues for cross-thread communication

This hybrid approach provides the best balance of development efficiency and runtime performance for the KoeLingo application.

## 3. Functional Requirements

### 3.1 Audio Capture
- Continuous audio capture from system microphone
- Configurable sample rate (16kHz recommended for Whisper)
- Adjustable buffer size to balance latency and processing load
- Audio level visualization for user feedback

### 3.2 Speech Recognition
- Real-time Japanese speech-to-text using local Whisper model
- Support for different Whisper model sizes (tiny through large)
- Transcription with timestamps for synchronization
- Speech detection confidence indicators
- Noise filtering and audio preprocessing

### 3.3 Translation
- Real-time Japanese to English translation
- Sentence-level translation to maintain context
- Translation confidence metrics
- Support for alternative translations

### 3.4 User Interface
- Clean, minimal two-panel design (Japanese & English)
- Visual microphone activity indicator
- Start/Stop listening controls
- Save & Close functionality
- Auto-saving capability at configurable intervals
- Speech confidence visualization
- Dark/Light mode toggle
- Text size adjustment

### 3.5 Data Storage
- Automatic saving of transcripts (both languages)
- Configurable auto-save intervals (default: 30 seconds)
- Export options (TXT, PDF, DOCX)
- Session recovery in case of unexpected closure

## 4. Non-Functional Requirements

### 4.1 Performance
- Maximum end-to-end latency: < 1 second
- Continuous operation capability: 2+ hours
- Minimal CPU/GPU utilization to prevent thermal throttling
- Memory usage < 4GB for full application

### 4.2 Reliability
- Graceful handling of audio device disconnections
- Auto-recovery from model inference errors
- Regular background saving of session data
- Crash reporting and diagnostics

### 4.3 Usability
- Intuitive, single-window interface
- Keyboard shortcuts for common operations
- Clear visual indicators of system status
- Minimal configuration required for first use

## 5. UI Design Specifications

### 5.1 Main Window
- Two-panel layout with equal sizing for Japanese and English text
- Microphone status indicator with visual audio level feedback
- Start/Stop button with clear status indication
- Save & Close button in accessible location
- Status bar with system information (model loaded, auto-save status)

### 5.2 Visual Indicators
- Microphone activity visualization (audio waveform or level meter)
- Speech detection confidence indicators (color-coding or percentage)
- Translation processing indicator
- Auto-save status notification

### 5.3 Settings Panel
- Model selection dropdown (Whisper model size)
- Auto-save interval configuration
- Export format options
- Audio device selection
- Theme toggle (Dark/Light)
- Font size adjustment

## 6. Component Architecture

### 6.1 Audio Module
- Responsible for microphone access and audio buffering
- Implements audio level metering
- Provides preprocessed audio chunks to STT module

### 6.2 Speech-to-Text Module
- Loads and manages Whisper model
- Processes audio chunks to generate Japanese text
- Provides confidence scores for recognized segments
- Handles continuous speech vs. silence detection

### 6.3 Translation Module
- Manages NLLB model for Japanese-English translation
- Processes Japanese text in appropriate chunks
- Returns translated English text with confidence metrics

### 6.4 UI Module
- Implements Qt-based user interface
- Manages text display and formatting
- Handles user input and control actions
- Displays status indicators and visualizations

### 6.5 Storage Module
- Manages regular auto-saving of transcripts
- Handles export functionality
- Implements session recovery mechanisms

## 7. Benchmarking Strategy

### 7.1 Audio Processing Metrics
- Measure audio capture latency at different sample rates/buffer sizes
- Test microphone input quality under varying noise conditions
- Benchmark memory usage during extended recording sessions

### 7.2 Speech-to-Text Performance
- Measure transcription accuracy for different Japanese speech patterns/accents
- Compare processing time between different Whisper model sizes
- Test real-time capabilities with continuous speech vs. batched processing

### 7.3 Translation Benchmarks
- Measure NLLB translation speed for different text lengths
- Compare translation quality against reference translations
- Evaluate memory usage during continuous translation

### 7.4 End-to-End System Tests
- Measure total latency from speech to displayed translation
- Test system stability during extended use (1+ hour sessions)
- Monitor CPU/GPU utilization and thermal performance

## 8. Implementation

The detailed implementation plan has been moved to a separate document:
[Implementation Plan](3.implementation.md)

This dedicated implementation document covers:
- Development approach and principles
- Detailed development phases and timeline
- Comprehensive testing strategy
- Phase-specific deliverables

## 9. Future Enhancements

### 9.1 Short-term
- Support for additional language pairs
- Customizable text highlighting and annotation
- Advanced export options (with timestamps)
- Basic editing capabilities for transcribed text

### 9.2 Long-term
- Speaker diarization (identifying different speakers)
- Integration with cloud services for backup
- Mobile companion application
- Support for specialized domain vocabulary

## 10. Development Environment Setup

### 10.1 Required Tools
- Python 3.9+ with virtual environment
- Qt Creator for UI design
- Git for version control
- PyTorch for model inference
- CTranslate2 for optimized inference (optional)

### 10.2 Development Workflow
- Feature branch workflow with pull requests
- Continuous integration for automated testing
- Regular performance benchmarking
- UI/UX review checkpoints